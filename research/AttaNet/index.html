

<!DOCTYPE html>
<html>

<head lang="en">
<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<title>Attention-Augmented Network</title>

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
<link rel="stylesheet" href="css/app.css">

<link rel="stylesheet" href="css/bootstrap.min.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
<script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
<div class="row">
<h2 class="col-md-12 text-center">
AttaNet: Attention-Augmented Network <br>
for Fast and Accurate Scene Parsing</br>
<!-- <small>
arXiv 2020
</small> -->
</h2>
</div>
<div class="row">
<div class="col-md-12 text-center">
<ul class="list-inline">
<li>
<a href="https://songqi-github.github.io/">
Qi Song
</a>
</br>CUHK(SZ)
</li>
<li>
<a href="https://mkfmiku.github.io/">
Kangfu Mei
</a>
</br>CUHK(SZ)
</li>
<li>
<a href="https://myweb.cuhk.edu.cn/ruihuang">
Rui Huang
</a>
</br>CUHK(SZ)
</li>
</ul>
*denotes equal contribution
</div>
</div>


<div class="row">
<div class="col-md-4 col-md-offset-4 text-center">
<ul class="nav nav-pills nav-justified">
<li>
<a href=" ">
<image src="./paper_image.png" height="60px">
<h4><strong>Paper</strong></h4>
</a>
</li>
<li>
<a href="https://github.com/songqi-github/AttaNet">
<image src="./github.png" height="60px">
<h4><strong>Code</strong></h4>
</a>
</li>
</ul>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Abstract
</h3>
<image src="./abstract.png" class="img-responsive" alt="overview"><br>
<p class="text-justify">
Two factors have proven to be very important to the performance of semantic segmentation models: global context and multi-level semantics. However, generating features that capture both factors always leads to high computational complexity, which is problematic in real-time scenarios. In this
paper, we propose a new model, called Attention-Augmented Network (AttaNet), to capture both global context and multi-level semantics while keeping the efficiency high. AttaNet
consists of two primary modules: Strip Attention Module (SAM) and Attention Fusion Module (AFM). Viewing that there is a significantly larger amount of vertical strip areas than horizontal ones in the natural images, SAM utilizes a striping operation to reduce the complexity of encoding
global context in the vertical direction drastically while keeping most of contextual information, compared to the non-local approaches. Moreover, AFM follows a cross-level aggregation strategy to limit the computation, and adopts an attention strategy to weight the importance of different levels
of features at each pixel when fusing them, obtaining an efficient multi-level representation. We have conducted extensive experiments on two semantic segmentation benchmarks, and our network achieves different levels of speed/accuracy tradeoff on Cityscapes, e.g., 71 FPS/79.9% mIoU, 130 FPS/78.5%
mIoU, and 180 FPS/70.1% mIoU, and leading performance on ADE20K as well.</p>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Training a network without and with SAM
</h3>
<image src="./SAM.png" class="img-responsive" alt="SAM"><br>
<p class="text-justify">
</p>
<image src="./wSAM.png" class="img-responsive" alt="wSAM"><br>
<p class="text-justify">
</p>
<p class="text-justify">
The benefits of our SAM are three-fold. First, since the striped feature map is the combination of all pixels along the same spatial dimension, this gives strong supervision in capturing anisotropy or banded context. 
  Second, we first ensure that the relationships between each pixel and all columns are considered, and then estimate the attention map along the horizontal axis, thus our network can generate dense contextual dependencies. 
  Moreover, this module adds only a few parameters to the backbone network, and therefore takes up very little GPU memory.
</p>
</div>
</div>


<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Training a network without and with AFM
</h3>
<image src="./wAFM.png" class="img-responsive" alt="wAFM"><br>
<p class="text-justify">
</p>
<p class="text-justify">
Since low-level features contain excessive spatial details while high-level features are rich in semantics, simply aggregating multi-level information would weaken the effectiveness of information propagation. 
  To address this issue, we introduce Attention Fusion Module which enables each pixel to choose individual contextual information from multi-level features in the aggregation phase. 
  AFM can exploit more discriminative context for each class.
</p>
</div>
</div>


</p>
</div>
</div>
</div>
</body>
</html>
