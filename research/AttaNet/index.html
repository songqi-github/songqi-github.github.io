

<!DOCTYPE html>
<html>

<head lang="en">
<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<title>Attention-Augmented Network</title>

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
<link rel="stylesheet" href="css/app.css">

<link rel="stylesheet" href="css/bootstrap.min.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
<script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
<div class="row">
<h2 class="col-md-12 text-center">
AttaNet: Attention-Augmented Network <br>
for Fast and Accurate Scene Parsing</br>
<!-- <small>
arXiv 2020
</small> -->
</h2>
</div>
<div class="row">
<div class="col-md-12 text-center">
<ul class="list-inline">
<li>
<a href="https://github.com/songqi-github">
Qi Song
</a>
</br>CUHK(SZ)
</li>
<li>
<a href="https://mkfmiku.github.io/">
Kangfu Mei
</a>
</br>CUHK(SZ)
</li>
<li>
<a href="https://myweb.cuhk.edu.cn/ruihuang">
Rui Huang
</a>
</br>CUHK(SZ)
</li>
</ul>
*denotes equal contribution
</div>
</div>


<div class="row">
<div class="col-md-4 col-md-offset-4 text-center">
<ul class="nav nav-pills nav-justified">
<li>
<a href=" ">
<image src="img/ff_paper_image.png" height="60px">
<h4><strong>Paper</strong></h4>
</a>
</li>
<li>
<a href="https://github.com/songqi-github">
<image src="img/github.png" height="60px">
<h4><strong>Code</strong></h4>
</a>
</li>
</ul>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Abstract
</h3>
<image src="img/teaser.png" class="img-responsive" alt="overview"><br>
<p class="text-justify">
Two factors have proven to be very important to the performance of semantic segmentation models: global context and multi-level semantics. However, generating features that capture both factors always leads to high computational complexity, which is problematic in real-time scenarios. In this
paper, we propose a new model, called Attention-Augmented Network (AttaNet), to capture both global context and multi-level semantics while keeping the efficiency high. AttaNet
consists of two primary modules: Strip Attention Module (SAM) and Attention Fusion Module (AFM). Viewing that there is a significantly larger amount of vertical strip areas than horizontal ones in the natural images, SAM utilizes a striping operation to reduce the complexity of encoding
global context in the vertical direction drastically while keeping most of contextual information, compared to the non-local approaches. Moreover, AFM follows a cross-level aggregation strategy to limit the computation, and adopts an attention strategy to weight the importance of different levels
of features at each pixel when fusing them, obtaining an efficient multi-level representation. We have conducted extensive experiments on two semantic segmentation benchmarks, and our network achieves different levels of speed/accuracy tradeoff on Cityscapes, e.g., 71 FPS/79.9% mIoU, 130 FPS/78.5%
mIoU, and 180 FPS/70.1% mIoU, and leading performance on ADE20K as well.</p>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Training a network without and with SAM
</h3>
<p class="text-justify">
</p>
<p class="text-justify">
</p>
<p style="text-align:center;">
<image src="img/wSAM.png" height="30px" class="center">
</p>
<p class="text-justify">
SAM generates more consistent segmentation inside large objects or along banded classes.
</p>
</div>
</div>


<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Training a network without and with AFM
</h3>
<p class="text-justify">
</p>
<p style="text-align:center;">
<image src="img/wAFM.png" height="30px" class="center">
</p>
<p class="text-justify">
AFM can exploit more discriminative context for each class.
</p>
</div>
</div>


</p>
</div>
</div>
</div>
</body>
</html>
