<!DOCTYPE html>
<html>

<head lang="en">
<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">

<title>Fully Attentional Network for Semantic Segmentation</title>

<meta name="description" content="">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
<link rel="stylesheet" href="css/app.css">

<link rel="stylesheet" href="css/bootstrap.min.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
<script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
<div class="row">
<h2 class="col-md-12 text-center">
Fully Attentional Network <br>
for Semantic Segmentation</br>
<!-- <small>
arXiv 2020
</small> -->
</h2>
</div>
<div class="row">
<div class="col-md-12 text-center">
<ul class="list-inline">
<li>
<a href="https://songqi-github.github.io/">
Qi Song
</a>
</br>CUHK(SZ)
</li>
<li>
Jie Li
</a>
</br>CUHK(SZ)
</li>
<li>
Chenghong Li
</a>
</br>CUHK(SZ)
</li>
<li>
Hao Guo
</a>
</br>CUHK(SZ)
</li>
<li>
<a href="https://myweb.cuhk.edu.cn/ruihuang">
Rui Huang
</a>
</br>CUHK(SZ)
</li>
</ul>
*denotes equal contribution
</div>
</div>


<div class="row">
<div class="col-md-4 col-md-offset-4 text-center">
<ul class="nav nav-pills nav-justified">
<li>
<a href="https://arxiv.org/pdf/2112.04108.pdf">
<image src="./paper_image.png" height="60px">
<h4><strong>Paper</strong></h4>
</a>
</li>
<li>
<a href="https://github.com/songqi-github/">
<image src="./github.png" height="60px">
<h4><strong>Code</strong></h4>
</a>
</li>
</ul>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Abstract
</h3>
<image src="./abstract.png" class="img-responsive" alt="overview"><br>
<p class="text-justify">
Recent non-local self-attention methods have proven to be effective in capturing long-range dependencies for semantic segmentation. These methods usually form a similarity map of \(\mathbb{R}^{C\times C}\) (by compressing spatial dimensions) or \(\mathbb{R}^{HW\times HW}\) (by compressing channels) to describe the feature relations along either channel or spatial dimensions, where \(C\) is the number of channels, \(H\) and \(W\) are the spatial dimensions of the input feature map. 
  However, such practices tend to condense feature dependencies along the other dimensions, hence causing attention missing, which might lead to inferior results for small/thin categories or inconsistent segmentation inside large objects. 
  To address this problem, we propose a new approach, namely Fully Attentional Network (FLANet), to encode both spatial and channel attentions in a single similarity map while maintaining high computational efficiency. Specifically, for each channel map, our FLANet can harvest feature responses from all other channel maps, and the associated spatial positions as well, through a novel fully attentional module. Our new method has achieved state-of-the-art performance on three challenging semantic segmentation datasets, i.e., 83.6\%, 46.99\%, and 88.5\% on the Cityscapes test set, the ADE20K validation set, and the PASCAL VOC test set, respectively.</p>
</div>
</div>



<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Motivation
</h3>
<image src="./nl_architecture.pdf" class="img-responsive" alt="NL"><br>
<p class="text-justify">
  Through the analysis of traditional NL methods, we found that one acute issue, i.e., \textbf{attention missing}, was mostly ignored. Take Channel NL for example, the channel attention map \(\mathbb{R}^{C\times C}\)  is generated by the matrix multiplication of two inputs with a dimension of \(C\times HW\) and \(HW\times C\). 
  It can be found that each channel can be connected with all other channel maps while the spatial information will be integrated and each spatial position fails to perceive feature response from other positions during the matrix multiplication. 
  Similarly, interactions among channel dimensions are also missing in the Spatial NL. We argue that the attention missing issue would damage the integrity of 3D context information (\(CHW\)) and thus both NL variants can only benefit partially in a complementary way. To verify this hypothesis, we carried out some quantitative studies.
</p>
<image src="./problem_visualization.pdf" class="img-responsive" alt="evidence"><br>
<p class="text-justify">
</p>
<p class="text-justify">
As shown in the figure, Channel NL gets better segmentation results among large objects, such as \textit{truck}, \textit{bus} and \textit{train}, while Spatial NL performs much better on small/thin categories, e.g., \textit{poles}, \textit{rider} and \textit{mbike}. 
  They both lose precision in some categories due to the mentioned attention missing issue. In addition, stacking two NL blocks in the network still suffers from the attention missing issue.
  However, it is observed that the performance of Dual NL drops a lot in large objects such as \textit{truck} and \textit{train}, and CS NL gets poor IoU results in some thin categories like \textit{pole} and \textit{mbike}. We can find that both Dual NL and CS NL can only preserve partial benefits brought by either Channel NL or Spatial NL. 
  Therefore, we can conclude that: the attention missing issue hurts the feature representation ability and it cannot be solved by simply stacking different NL blocks.
</p>
</div>
</div>


<div class="row">
<div class="col-md-8 col-md-offset-2">
<h3>
Model Architecture
</h3>
<image src="./fla.pdf" class="img-responsive" alt="FLA"><br>
<p class="text-justify">
</p>
<p class="text-justify">
Motivated by this, we propose a novel non-local block namely Fully Attentional block (FLA) to efficiently retain attentions in all dimensions. And the workflow is shown in Fig.\ref{fig1} (c). The basic idea is to utilize the global context information to receive spatial responses when computing the channel attention map, which enables full attentions in a single attention unit with high computational efficiency. 
  Specifically, we first enable each spatial position to harvest feature responses from the global contexts with the same horizontal and vertical coordinates. Second, we use the self-attention mechanism to capture the fully attentional similarities between any two channel maps and the associated spatial positions. 
  Finally, the generated fully attentional similarities are used to re-weight each channel map by integrating features among all channel maps and associated global clues. 
</p>
</div>
</div>


</p>
</div>
</div>
</div>
</body>
</html>
